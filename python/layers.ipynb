{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuOUTHGyR1OZ"
      },
      "source": [
        "# Практическое задание 2\n",
        "\n",
        "\n",
        "\n",
        "## Замечания\n",
        "\n",
        "* Задание необходимо сдать боту до 06.12.2021\n",
        "* Соблюдаем кодекс чести (по нулям и списавшему, и давшему списать)\n",
        "* Можно (и нужно!) применять для реализации только библиотеку **Numpy**\n",
        "* Ничего, крому Numpy, нельзя использовать для реализации \n",
        "* **Keras** используется только для тестирования Вашей реализации\n",
        "* Если какой-то из классов не проходит приведенные тесты, то соответствующее задание не оценивается\n",
        "* Возможно использование дополнительных (приватных) тестов\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4H9YMxvR1Oe"
      },
      "source": [
        "## Реализация собственного нейросетевого пакета для запуска и обучения нейронных сетей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3wsg4K_R1Oe"
      },
      "source": [
        "Задание состоит из трёх частей:\n",
        "1. Реализация прямого вывода нейронной сети (первое практическое задание)\n",
        "2. Реализация градиентов по входу и распространения градиента по сети (back propagation)\n",
        "3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30eO8eDJR1Of"
      },
      "source": [
        "###  1. Реализация вывода собственной нейронной сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFUJYwskR1Of"
      },
      "source": [
        "1.1 Внимательно ознакомьтесь с интерфейсом слоя. Любой слой должен содержать как минимум три метода:\n",
        "- конструктор\n",
        "- прямой вывод \n",
        "- обратный вывод, производные по входу и по параметрам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGDp57wxR1Og"
      },
      "source": [
        "class Layer(object):\n",
        "    def __init__(self):\n",
        "        self.name = 'Layer'       \n",
        "    def forward(self, input_data):\n",
        "        pass\n",
        "    def backward(self, input_data):\n",
        "        return [self.grad_x(input_data), self.grad_param(input_data)]\n",
        "    \n",
        "    def grad_x(self, input_data):\n",
        "        pass\n",
        "    def grad_param(self, input_data):\n",
        "        return []\n",
        "    \n",
        "    def update_param(self, grads, learning_rate):\n",
        "        pass\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvgq5a9NR1Oh"
      },
      "source": [
        "1.2 Ниже предствален интерфейс класса  Network. Обратите внимание на реализацию метода predict, который последовательно обрабатывает входные данные слой за слоем."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBtrhe1vR1Oi"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, layers, loss=None):\n",
        "        self.name = 'Network'\n",
        "        self.layers = layers\n",
        "        self.loss = loss\n",
        "    \n",
        "    def forward(self, input_data):\n",
        "        return self.predict(input_data)\n",
        "\n",
        "    def grad_x(self, input_data, labels):\n",
        "        grads_x = []\n",
        "        input = input_data\n",
        "        for layer in self.layers:\n",
        "            grads_x.append(layer.grad_x(input))\n",
        "            input = layer.forward(input) \n",
        "        out = np.reshape(self.loss.grad_x(input, labels), (self.loss.grad_x(input, labels).shape[0], 1, self.loss.grad_x(input, labels).shape[1]))\n",
        "        for out_layer in reversed(grads_x):\n",
        "            out = np.matmul(out, out_layer)\n",
        "        return np.reshape(out, (out.shape[0], out.shape[2]))\n",
        "\n",
        "    def grad_param(self, input_data, labels):\n",
        "        out = []\n",
        "        input = input_data\n",
        "        for layer in self.layers:\n",
        "            out.append(layer.backward(input))\n",
        "            input = layer.forward(input) \n",
        "        out.append([self.loss.grad_x(input, labels), []])\n",
        "        return out\n",
        "\n",
        "    def update(self, grad_list, learning_rate):\n",
        "        grad_x = grad_list[-1][0]\n",
        "        grad_x = np.reshape(grad_x, (grad_x.shape[0], 1, grad_x.shape[1]))\n",
        "        for i in range(len(grad_list)-2, -1, -1):\n",
        "            if len(grad_list[i][1]) != 0:\n",
        "                self.layers[i].update_param([np.matmul(grad_x, grad_list[i][1][0]), np.matmul(grad_x, grad_list[i][1][1])], learning_rate)\n",
        "            grad_x = np.matmul(grad_x, grad_list[i][0])\n",
        "    \n",
        "    \n",
        "    def predict(self, input_data):\n",
        "        current_input = input_data\n",
        "        for layer in self.layers:\n",
        "            current_input = layer.forward(current_input)     \n",
        "        return current_input\n",
        "    \n",
        "    def calculate_loss(self, input_data, labels):\n",
        "        return self.loss.forward(self.predict(input_data), labels)\n",
        "    \n",
        "    def train_step(self, input_data, labels, learning_rate=0.001):\n",
        "        grad_list = self.grad_param(input_data, labels)\n",
        "        self.update(grad_list, learning_rate)\n",
        "    \n",
        "    def fit(self, trainX, trainY, validation_split=0.25, \n",
        "            batch_size=1, nb_epoch=1, learning_rate=0.01):\n",
        "        \n",
        "        train_x, val_x, train_y, val_y = train_test_split(trainX, trainY, \n",
        "                                                          test_size=validation_split,\n",
        "                                                          random_state=42)\n",
        "        for epoch in range(nb_epoch):\n",
        "            #train one epoch\n",
        "            for i in tqdm(range(int(len(train_x)/batch_size))):\n",
        "                batch_x = train_x[i*batch_size: (i+1)*batch_size]\n",
        "                batch_y = train_y[i*batch_size: (i+1)*batch_size]\n",
        "                self.train_step(batch_x, batch_y, learning_rate)\n",
        "            #validate\n",
        "            val_accuracy = self.evaluate(val_x, val_y)\n",
        "            print(\"\\n\",'%d epoch: val %.2f' %(epoch+1, val_accuracy), \"\\n\")\n",
        "            \n",
        "    def evaluate(self, testX, testY):\n",
        "        y_pred = np.argmax(self.predict(testX), axis=1)            \n",
        "        y_true = np.argmax(testY, axis=1)\n",
        "        val_accuracy = np.sum((y_pred == y_true))/(len(y_true))\n",
        "        return val_accuracy"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGGs9_tkR1Oj"
      },
      "source": [
        "#### 1.1 (6 баллов) Необходимо реализовать метод forward для вычисления следующих слоёв:\n",
        "\n",
        "- DenseLayer\n",
        "- ReLU\n",
        "- Softmax\n",
        "- FlattenLayer\n",
        "- MaxPooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98A0fEXyR1Ok"
      },
      "source": [
        "#импорты\n",
        "import numpy as np"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tojxogmR1Ol"
      },
      "source": [
        "class DenseLayer(Layer):\n",
        "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None):\n",
        "        self.name = 'Dense'\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        if W_init is None or b_init is None:\n",
        "            self.W = np.random.normal(0, np.sqrt(2. / input_dim), size = (input_dim, output_dim))\n",
        "            self.b = np.zeros(output_dim, 'float32')\n",
        "        else:\n",
        "            self.W = W_init\n",
        "            self.b = b_init\n",
        "    def forward(self, input_data):\n",
        "        return np.matmul(input_data, self.W) + self.b\n",
        "    def grad_x(self, input_data):\n",
        "        out = np.zeros(shape = (input_data.shape[0], self.output_dim, self.input_dim))\n",
        "        for o in out:\n",
        "          for i in range(self.output_dim):\n",
        "            for j in range(self.input_dim):\n",
        "                o[i, j] = self.W[j, i]\n",
        "        return out\n",
        "    def grad_b(self, input_data):\n",
        "        out = np.zeros(shape = (input_data.shape[0], self.output_dim, self.output_dim))\n",
        "        for o in out:\n",
        "            for i in range(self.output_dim):\n",
        "                o[i, i] = 1\n",
        "        return out \n",
        "    def grad_W(self, input_data):\n",
        "        out = np.zeros(shape = (input_data.shape[0], self.output_dim, self.input_dim * self.output_dim))\n",
        "        k=0\n",
        "        for o in out:\n",
        "          for i in range(self.output_dim):\n",
        "            for j in range(self.input_dim):\n",
        "                o[i, j*self.output_dim + i] = input_data[k, j]\n",
        "          k +=1\n",
        "        return out\n",
        "    \n",
        "    def update_W(self, grad, learning_rate):\n",
        "        self.W -= learning_rate * np.mean(grad, axis=0).reshape(self.W.shape)\n",
        "    \n",
        "    def update_b(self, grad,  learning_rate):\n",
        "        self.b -= learning_rate * np.mean(grad, axis=0).reshape(self.b.shape)\n",
        "        \n",
        "    def update_param(self, params_grad, learning_rate):\n",
        "        self.update_W(params_grad[0], learning_rate)\n",
        "        self.update_b(params_grad[1], learning_rate)\n",
        "    \n",
        "    def grad_param(self, input_data):\n",
        "        return [self.grad_W(input_data), self.grad_b(input_data)]\n",
        "    \n",
        "\n",
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'ReLU'\n",
        "    def forward(self, input_data):\n",
        "      out = np.zeros(input_data.shape)\n",
        "      for (index, x) in np.ndenumerate(input_data):\n",
        "        out[index] = x if x > 0 else 0\n",
        "      return out\n",
        "    def grad_x(self, input_data):\n",
        "        out = np.zeros((input_data.shape[0], input_data.shape[1], input_data.shape[1]))\n",
        "        for b in range(input_data.shape[0]):\n",
        "            for i in range(input_data.shape[1]):\n",
        "              out[b][i][i] =  1 if (input_data[b][i] > 0) else 0;\n",
        "        return out \n",
        "    \n",
        "    \n",
        "class Softmax(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'Softmax'\n",
        "    def forward(self, input_data):\n",
        "      out = np.zeros(input_data.shape)\n",
        "      for batch in range(input_data.shape[0]):\n",
        "        out[batch] = np.exp(input_data[batch]) / np.sum(np.exp(input_data[batch]))\n",
        "      return out\n",
        "    def grad_x(self, input_data):\n",
        "        out = np.zeros(shape = (input_data.shape[0], input_data.shape[1], input_data.shape[1]))\n",
        "        for batch in range(input_data.shape[0]):\n",
        "            expon = np.exp(input_data[batch])\n",
        "            for i in range(input_data.shape[1]):\n",
        "                for j in range(input_data.shape[1]):\n",
        "                    out[batch, i, j] = - (expon[i] * expon[j]) / np.sum(expon)**2 + (expon[i] / np.sum(expon) if i == j else 0) \n",
        "        return out\n",
        "    \n",
        "\n",
        "class FlattenLayer(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'Flatten'\n",
        "    def forward(self, input_data):\n",
        "      out = np.zeros([input_data.shape[0], input_data.shape[1] * input_data.shape[2] * input_data.shape[3]])\n",
        "      for batch in range(input_data.shape[0]):\n",
        "          counter = 0\n",
        "          for j in range(input_data.shape[2]):\n",
        "              for k in range(input_data.shape[3]):\n",
        "                  for i in range(input_data.shape[1]):\n",
        "                      out[batch, counter] = input_data[batch, i, j, k]\n",
        "                      counter += 1\n",
        "      return out\n",
        "    def grad_x(self, input_data):\n",
        "      out = np.zeros((input_data.shape[0], input_data.shape[1] * input_data.shape[2] * input_data.shape[3], input_data.shape[1] * input_data.shape[2] * input_data.shape[3]))\n",
        "      for batch in range(input_data.shape[0]):\n",
        "        for i in range(input_data.shape[1] * input_data.shape[2] * input_data.shape[3]):\n",
        "          out[batch, i, i] = 1\n",
        "      return out\n",
        "\n",
        "\n",
        "class MaxPooling(Layer):\n",
        "    def __init__(self, pool_size=(2, 2), strides=2):\n",
        "        self.name = 'MaxPooling'\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = 2\n",
        "    def forward(self, input_data):\n",
        "      out = np.zeros([input_data.shape[0], input_data.shape[1],  (input_data.shape[2] - self.pool_size + self.stride)//self.stride, (input_data.shape[3] - self.pool_size + self.stride)//self.stride])\n",
        "      for batch in range(input_data.shape[0]):\n",
        "          for ch in range(input_data.shape[1]):\n",
        "              for x in range(0, input_data.shape[2] - self.pool_size + 1, self.stride):\n",
        "                  for y in range(0, input_data.shape[3] - self.pool_size + 1, self.stride):\n",
        "                      out[batch, ch, x // self.strides, y // self.strides] = np.amax(input_data[batch, ch, x:x + self.pool_size, y:y + self.pool_size])\n",
        "      return out    \n",
        "    def grad_x(self, input_data):\n",
        "        (p1, p2) = self.pool_size\n",
        "        out = np.zeros([input_data.shape[0], input_data.shape[1]*(input_data.shape[2] - p1 +self.stride) //self.stride*(input_data.shape[3] - p2 +self.stride) //self.stride, input_data.shape[1]*input_data.shape[2]*input_data.shape[3]])\n",
        "        for batch in range(input_data.shape[0]):\n",
        "          for channel in range(input_data.shape[1]):\n",
        "            for x in range(0, input_data.shape[2] - p1 + 1,self.stride):\n",
        "              for y in range(0, input_data.shape[3] - p2 + 1,self.stride):\n",
        "                for i in range(x,x+p1):\n",
        "                  for j in range(y,y+p2):\n",
        "                    out[batch, channel*(input_data.shape[2] - p1 +self.stride) //self.stride*(input_data.shape[3] - p2 +self.stride) //self.stride + (x//s)*(input_data.shape[3] - p2 +self.stride) //self.stride + (y//s), channel*input_data.shape[2]*input_data.shape[3] + i*input_data.shape[3] + j] = 1 if input_data[batch, channel, i, j] == np.amax(input_data[batch, channel, x:x+p1, y:y+p2]) else 0\n",
        "        return out    "
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1quSgzvR1Ol"
      },
      "source": [
        "#### 1.2 (3 балла) Реализуйте теперь свёртночный слой   (опционально)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isU5FhifXr3p"
      },
      "source": [
        "class Conv2D(Layer):\n",
        "    def __init__(self, kernel_size, input_channels, output_channels, \n",
        "                 kernels_init=None, bias_init=None):\n",
        "        self.name = 'Conv2D'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.input_channels = input_channels\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        if kernels_init is None or bias_init is None:\n",
        "          self.kernel = np.random.normal(0, np.sqrt(2. / (kernel_size*kernel_size*output_channels)), size =  (kernel_size,kernel_size,input_channels,output_channels))\n",
        "          self.bias = np.zeros(output_channels, 'float32')\n",
        "        else:\n",
        "          self.kernels = kernels_init\n",
        "          self.bias = bias_init\n",
        "            \n",
        "    def forward(self, input_data):\n",
        "      def point(x,y):\n",
        "        return (np.moveaxis(x, 0, -1)*y).sum()\n",
        "\n",
        "      input = input_data.copy()\n",
        "      if self.padding == 'same':\n",
        "          input = np.pad(input, pad_width=((0, 0), (0, 0), (max(self.kernel_size - (input.shape[2] - 1) % self.stride - 1, 0) // 2, max(self.kernel_size - (input.shape[2] - 1) % self.stride - 1, 0)-(max(self.kernel_size - (input.shape[2] - 1) % self.stride - 1, 0) // 2)),\n",
        "                        (max(self.kernel_size - (input.shape[3] - 1) % self.stride - 1, 0) // 2,  max(self.kernel_size - (input.shape[3] - 1) % self.stride - 1, 0) - max(self.kernel_size - (input.shape[3] - 1) % self.stride - 1, 0) // 2)), mode='constant',\n",
        "                        constant_values=0)\n",
        "      out = np.zeros([input.shape[0], self.output_channels, (input.shape[2] - self.kernel_size + 1 + self.stride - 1) // self.stride, (input.shape[3] - self.kernel_size + 1 + self.stride - 1) // self.stride])\n",
        "      for batch in range(input.shape[0]):\n",
        "          for channel in range(self.output_channels):\n",
        "              for x in range(0, input.shape[2] - self.kernel_size + 1, self.stride):\n",
        "                  for y in range(0, input.shape[3] - self.kernel_size + 1, self.stride):\n",
        "                      a = input[batch, :, x:x + self.kernel_size, y:y + self.kernel_size]\n",
        "                      b = self.kernel[:, :, :, channel]\n",
        "                      out[batch, channel, x // self.stride, y // self.stride] = point(a,b) \\\n",
        "                          + self.bias[channel]\n",
        "      return out\n",
        "    def grad_x(self):\n",
        "        pass\n",
        "    def grad_kernel(self):\n",
        "        pass"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJeo-b3_R1Om"
      },
      "source": [
        "#### 1.4 Теперь настало время теста. \n",
        "#### Если вы всё сделали правильно, то запустив следующие ячейки у вас должна появиться надпись: Test PASSED\n",
        "\n",
        "Переходить к дальнейшим заданиям не имеем никакого смысла, пока вы не добьётесь прохождение теста\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-7IFb6rR1Om"
      },
      "source": [
        "#### Чтение данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x55GwDyFR1On",
        "outputId": "f37ffa9e-7bcc-4250-90fb-6bb8408799e9"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(123)  # for reproducibility\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        " \n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        " \n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CUeQQ7YR1Oo"
      },
      "source": [
        "#### Подготовка моделей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9EzhAa9R1Op"
      },
      "source": [
        "### 2. Вычисление производных по входу для слоёв нейронной сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HN3iyKDR1Op"
      },
      "source": [
        "#### 2.1 (1 балл) Реализуйте метод forward для класса CrossEntropy\n",
        "Напоминание: $$ crossentropy = L(p, y) =  - \\sum\\limits_i y_i log p_i, $$\n",
        "где вектор $(p_1, ..., p_k) $ -  выход классификационного алгоритма, а $(y_1,..., y_k)$ - правильные метки класса в унарной кодировке (one-hot encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNSfBzAUR1Oq"
      },
      "source": [
        "class CrossEntropy(object):\n",
        "    def __init__(self, eps=0.00001):\n",
        "        self.name = 'CrossEntropy'\n",
        "        self.eps = eps\n",
        "    \n",
        "    def forward(self, input_data, labels):\n",
        "        log = np.log(input_data + self.eps)\n",
        "        out = - np.sum(labels * log, axis = 1)\n",
        "        return out\n",
        "    \n",
        "    def calculate_loss(self,input_data, labels):\n",
        "        return self.forward(input_data, labels)\n",
        "    \n",
        "    def grad_x(self, input_data, labels):\n",
        "        out = np.zeros_like(input_data, dtype = np.float64)\n",
        "        for batch in range(input_data.shape[0]):\n",
        "          out[batch] = (-labels[batch] / (input_data[batch]+self.eps))\n",
        "        return out"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNOY8iw7R1Oq"
      },
      "source": [
        "#### 2.2 (2 баллa) Реализуйте метод grad_x класса CrossEntropy, который возвращает $\\frac{\\partial L}{\\partial p}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfSP4v2VR1Oq"
      },
      "source": [
        "Проверить работоспособность кода поможет следующий тест:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfaOsOe4R1Oq",
        "outputId": "5b74a0b1-9050-4eb4-9490-ab545ce84e42"
      },
      "source": [
        "def numerical_diff_net(net, x, labels):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(x[0])):\n",
        "        delta = np.zeros(len(x[0]))\n",
        "        delta[i] = eps\n",
        "        diff = (net.calculate_loss(x + delta, labels) - net.calculate_loss(x-delta, labels)) / (2*eps)\n",
        "        right_answer.append(diff)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_net(net):\n",
        "    x = np.array([[1, 2, 3], [2, 3, 4]])\n",
        "    labels = np.array([[0.3, 0.2, 0.5], [0.3, 0.2, 0.5]])\n",
        "    num_grad = numerical_diff_net(net, x, labels)\n",
        "    grad = net.grad_x(x, labels)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "        \n",
        "loss = CrossEntropy()\n",
        "test_net(loss)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mas1G_6gR1Oq"
      },
      "source": [
        "#### 2.3 (2 балла)   Реализуйте метод grad_x класса Softmax, который возвращает $\\frac{\\partial Softmax}{\\partial x}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-pJuMD-R1Or"
      },
      "source": [
        "Проверить работоспособность кода поможет следующий тест:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZthzlKUcR1Or",
        "outputId": "033d437f-78e4-4470-8621-c4a5be6baa0a"
      },
      "source": [
        "def numerical_diff_layer(layer, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(x[0])):\n",
        "        delta = np.zeros(len(x[0]))\n",
        "        delta[i] = eps\n",
        "        diff = (layer.forward(x + delta) - layer.forward(x-delta)) / (2*eps)\n",
        "        right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_layer(layer):\n",
        "    x = np.array([[1, 2, 3], [2, -3, 4]])\n",
        "    num_grad = numerical_diff_layer(layer, x)\n",
        "    grad = layer.grad_x(x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "        \n",
        "layer = Softmax()\n",
        "test_layer(layer)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jn587J6R1Or"
      },
      "source": [
        "#### 2.4 (5 баллов) Реализуйте метод grad_x для классов ReLU и DenseLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fByOumwR1Or",
        "outputId": "5ff64fd6-00d6-4812-d2a9-9e2d442e5fe7"
      },
      "source": [
        "layer = ReLU()\n",
        "test_layer(layer)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWE6_F2YR1Or",
        "outputId": "9cc2bbbb-6f96-43c3-84ba-e0060e7ef4d2"
      },
      "source": [
        "layer = DenseLayer(3,4)\n",
        "test_layer(layer)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XFt2qXYR1Os"
      },
      "source": [
        "#### 2.5 (4 балла) Для класса Network реализуйте метод grad_x, который должен реализовывать взятие производной от лосса по входу"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5htmL9lR1Os",
        "outputId": "730dd013-5967-4e5d-feff-067c5a22b7b0"
      },
      "source": [
        "net = Network([DenseLayer(3, 10), ReLU(), DenseLayer(10, 3), Softmax()], loss=CrossEntropy())\n",
        "test_net(net)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LNnSrjBR1Os"
      },
      "source": [
        "### 3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWPXT9EmR1Os"
      },
      "source": [
        "#### 3.1 (4 балла) Реализуйте функции grad_b и grad_W. При подготовке теста grad_W предполагается, что W является отномерным вектором."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgWhlycoR1Os",
        "outputId": "40913169-e9c6-47eb-8102-129faf1b6386"
      },
      "source": [
        "def numerical_grad_b(input_size, output_size, b, W, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(b)):\n",
        "        delta = np.zeros(b.shape)\n",
        "        delta[i] = eps\n",
        "        dense1 = DenseLayer(input_size, output_size, W_init=W, b_init=b+delta)\n",
        "        dense2 = DenseLayer(input_size, output_size, W_init=W, b_init=b-delta)\n",
        "        diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
        "        right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_grad_b():\n",
        "    input_size = 3\n",
        "    output_size = 4 \n",
        "    W_init = np.random.random((input_size, output_size))\n",
        "    b_init = np.random.random((output_size,))\n",
        "    x = np.random.random((2, input_size))\n",
        "    \n",
        "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
        "    grad = dense.grad_b(x)\n",
        "\n",
        "    num_grad = numerical_grad_b(input_size, output_size, b_init, W_init, x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "test_grad_b()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzi0e9KtR1Ov",
        "outputId": "ec3cc3bb-9e7e-4637-fd55-619d3ce60912"
      },
      "source": [
        "def numerical_grad_W(input_size, output_size, b, W, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            delta = np.zeros(W.shape)\n",
        "            delta[i, j] = eps\n",
        "            dense1 = DenseLayer(input_size, output_size, W_init=W+delta, b_init=b)\n",
        "            dense2 = DenseLayer(input_size, output_size, W_init=W-delta, b_init=b)\n",
        "            diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
        "            right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_grad_W():\n",
        "    input_size = 3\n",
        "    output_size = 4 \n",
        "    W_init = np.random.random((input_size, output_size))\n",
        "    b_init = np.random.random((4,))\n",
        "    x = np.random.random((2, input_size))\n",
        "        \n",
        "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
        "    grad = dense.grad_W(x)\n",
        "\n",
        "    num_grad = numerical_grad_W(input_size, output_size, b_init, W_init, x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "test_grad_W()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP92HI7gR1Ov"
      },
      "source": [
        "#### 3.2 (4 балла) Полностью реализуйте метод обратного распространения ошибки в функции train_step класса Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDYqkDYPR1Ov"
      },
      "source": [
        "Рекомендуем реализовать сначала функцию Network.grad_param(), которая возвращает список длиной в количество слоёв и элементом которого является список градиентов по параметрам.\n",
        "После чего, имея список градиентов, написать функцию обновления параметров для каждого слоя. \n",
        "\n",
        "Совет: рекомендуем написать тест для кода подсчета градиента по параметрам, чтобы быть уверенным в том, что градиент через всю сеть считается правильно\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6dDBasZR1Ov"
      },
      "source": [
        "#### 3.3 Ознакомьтесь с реализацией функции fit класса Network. Запустите обучение модели. Если всё работает правильно, то точность на валидации должна будет возрастать"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFb7tS7nSAMi",
        "outputId": "5faecc66-dfcb-47de-e64a-10956739d9ec"
      },
      "source": [
        "net = Network([DenseLayer(784, 10), Softmax()], loss=CrossEntropy())\n",
        "trainX = X_train.reshape(len(X_train), -1)\n",
        "net.fit(trainX[::3], Y_train[::3], validation_split=0.25, \n",
        "            batch_size=16, nb_epoch=10, learning_rate=0.01)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:47<00:00,  5.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 1 epoch: val 0.85 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:50<00:00,  5.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 2 epoch: val 0.87 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:46<00:00,  5.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 3 epoch: val 0.88 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:48<00:00,  5.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 4 epoch: val 0.89 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:46<00:00,  5.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 5 epoch: val 0.89 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:48<00:00,  5.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 6 epoch: val 0.89 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:46<00:00,  5.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 7 epoch: val 0.90 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:48<00:00,  5.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 8 epoch: val 0.90 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:46<00:00,  5.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 9 epoch: val 0.90 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:46<00:00,  5.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 10 epoch: val 0.90 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKA17BdXR1Ow"
      },
      "source": [
        "#### 3.5 (2 балла) Продемонстрируйте, что ваша реализация позволяет обучать более глубокие нейронные сети "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEMGGEviG0B4",
        "outputId": "3f523ad6-70b3-4918-c7ab-88fbe23fdd5c"
      },
      "source": [
        "net = Network([DenseLayer(784, 200), ReLU(), DenseLayer(200, 100), ReLU(), DenseLayer(100, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
        "trainX = X_train.reshape(len(X_train), -1)\n",
        "net.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n",
        "            batch_size=16, nb_epoch=5, learning_rate=0.01)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [31:50<00:00,  4.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 1 epoch: val 0.83 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [31:37<00:00,  4.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 2 epoch: val 0.87 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [31:39<00:00,  4.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 3 epoch: val 0.89 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 39/468 [02:38<29:41,  4.15s/it]"
          ]
        }
      ]
    }
  ]
}